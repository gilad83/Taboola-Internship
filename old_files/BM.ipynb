{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras import backend\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import argparse\n",
    "\n",
    "name_to_shortcut = {\n",
    "    \"avg_memory\":\"avg_memory\",\n",
    "    \"cpu_user_util\":\"cpu_user_util\",\n",
    "    \"gc_time\":\"gc_time\",\n",
    "    \"load_score_meter\":\"load_score_meter\",\n",
    "    \"max_heap\":\"max_heap\",\n",
    "    \"p99_response_time\":\"p99_response_time\",\n",
    "    \"reco_rate\":\"reco_rate\"\n",
    "}\n",
    "name_to_shortcut1 = {\n",
    "    \"recommendation_requests_5m_rate_dc\": \"rec_5m\",\n",
    "    \"total_failed_action_conversions\": \"failed\",\n",
    "    \"total_success_action_conversions\": \"success\",\n",
    "    \"trc_requests_timer_p95_weighted_dc\": \"p95\",\n",
    "    \"trc_requests_timer_p99_weighted_dc\": \"p99\",\n",
    "    \"num_of_requests\": \"requests\"\n",
    "    # \"cpu_user_util\":\"cpu_user_util\"\n",
    "}\n",
    "\n",
    "\n",
    "class Reader:\n",
    "    # path is the path to the Main dataset folder\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    # set the path again if you want to use the same reader to read another dataset\n",
    "    def set_path(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    # import all the data to one dataset\n",
    "    def import_data(self):\n",
    "        dirs = [x[0] for x in os.walk(self.path)]\n",
    "        dirs.remove(self.path)\n",
    "        self.dir_names = [dir.replace(self.path + os.path.sep, '') for dir in dirs]\n",
    "        data_frames = []\n",
    "        for dir in dirs:\n",
    "            df = pd.concat(\n",
    "                [pd.read_csv(os.path.join(dir, x)) for x in os.listdir(dir) if os.path.isfile(os.path.join(dir, x))])\n",
    "            df.columns = ['date', dir.replace(self.path + os.path.sep, '')]\n",
    "            data_frames.append(df)\n",
    "        # merge\n",
    "        dataset = reduce(lambda left, right: pd.merge(left, right, on='date'), data_frames)\n",
    "        dataset.drop_duplicates(subset=None, inplace=True)\n",
    "        dates = dataset['date']\n",
    "        dataset.drop('date', 1)\n",
    "        dataset.drop(dataset.columns[[0]], axis=1, inplace=True)\n",
    "        return dates, dataset\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fetch_data(self, path):\n",
    "        r = Reader(path)\n",
    "        self.dates, self.raw_dataset = r.import_data()\n",
    "        self.feacher_names = r.dir_names\n",
    "\n",
    "    def add_features(self, data_point_to_predict, prediction):\n",
    "        self.add_trend()\n",
    "        self.add_multiply(data_point_to_predict, prediction)\n",
    "        self.add_is_weekend()\n",
    "        self.add_day_in_week()\n",
    "        #self.add_is_rush_hour()\n",
    "        # self.drop_low_corr_feature(prediction)\n",
    "\n",
    "    def add_trend(self):\n",
    "        feature_names = self.feacher_names.copy()\n",
    "        i = 0\n",
    "        for feature in feature_names:\n",
    "            i += 1\n",
    "            x = self.raw_dataset[feature]\n",
    "            trend = [b - a for a, b in zip(x[::1], x[1::1])]\n",
    "            trend.append(0)\n",
    "            self.raw_dataset[\"trend \" + name_to_shortcut[feature]] = trend\n",
    "            self.feacher_names.append(\"trend \" + name_to_shortcut[feature])\n",
    "\n",
    "\n",
    "    def add_is_rush_hour(self):\n",
    "        # requests = self.raw_dataset['recommendation_requests_5m_rate_dc']\n",
    "        requests = self.raw_dataset['reco_rate']\n",
    "        threshold_value_1 = requests.sort_values()[math.floor(0.9 * requests.size)]\n",
    "        threshold_value_2 = requests.sort_values()[math.floor(0.8 * requests.size)]\n",
    "        threshold_value_3 = requests.sort_values()[math.floor(0.7 * requests.size)]\n",
    "        threshold_value_4 = requests.sort_values()[math.floor(0.6 * requests.size)]\n",
    "        threshold_value_5 = requests.sort_values()[math.floor(0.5 * requests.size)]\n",
    "\n",
    "        is_rush_hour1 = [1 if num > threshold_value_1 else 0 for num in requests]\n",
    "        is_rush_hour2 = [1 if num > threshold_value_2 else 0 for num in requests]\n",
    "        is_rush_hour3 = [1 if num > threshold_value_3 else 0 for num in requests]\n",
    "        is_rush_hour4 = [1 if num > threshold_value_4 else 0 for num in requests]\n",
    "        is_rush_hour5 = [1 if num > threshold_value_5 else 0 for num in requests]\n",
    "\n",
    "        self.raw_dataset['is_rush_hour1'] = is_rush_hour1\n",
    "        self.raw_dataset['is_rush_hour2'] = is_rush_hour2\n",
    "        self.raw_dataset['is_rush_hour3'] = is_rush_hour3\n",
    "        self.raw_dataset['is_rush_hour4'] = is_rush_hour4\n",
    "        self.raw_dataset['is_rush_hour5'] = is_rush_hour5\n",
    "\n",
    "        self.feacher_names.append('is_rush_hour1')\n",
    "        self.feacher_names.append('is_rush_hour2')\n",
    "        self.feacher_names.append('is_rush_hour3')\n",
    "        self.feacher_names.append('is_rush_hour4')\n",
    "        self.feacher_names.append('is_rush_hour5')\n",
    "\n",
    "    def add_day_in_week(self):\n",
    "        dates = pd.to_datetime(self.dates, format='%Y-%m-%dT%H:%M:%S')\n",
    "        day_in_week = [date.weekday() for date in dates]\n",
    "        self.raw_dataset['day_in_week'] = day_in_week\n",
    "        self.feacher_names.append('day_in_week')\n",
    "\n",
    "    def add_is_weekend(self):\n",
    "        dates = pd.to_datetime(self.dates, format='%Y-%m-%dT%H:%M:%S')\n",
    "        is_weekend = [1 if date.weekday() >= 5 else 0 for date in dates]\n",
    "        self.raw_dataset['is_weekend'] = is_weekend\n",
    "        self.feacher_names.append('is_weekend')\n",
    "\n",
    "    def heat_map(self):\n",
    "        fig, ax = plt.subplots(figsize=(11, 11))\n",
    "        sns.heatmap(self.raw_dataset.corr(), cmap='coolwarm')\n",
    "        plt.savefig('heat_map.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    def drop_low_corr_feature(self, prediction):\n",
    "        corr = self.raw_dataset.corr()[prediction].copy()\n",
    "        corr = corr.abs()\n",
    "        print(corr.sort_values())\n",
    "        feature_names = self.feacher_names.copy()\n",
    "        for name in feature_names:\n",
    "            if (corr[name] < 0.4):\n",
    "                self.feacher_names.remove(name)\n",
    "                self.raw_dataset.drop(columns=[name], inplace=True)\n",
    "\n",
    "    def add_multiply(self, data_point_to_predict, prediction):\n",
    "        if data_point_to_predict == 0:\n",
    "            self.feacher_names.remove(prediction)\n",
    "        feature_names1 = self.feacher_names.copy()\n",
    "        feature_names2 = self.feacher_names.copy()\n",
    "        for feature1 in feature_names1:\n",
    "            for feature2 in feature_names2:\n",
    "                if ((feature1 != feature2) and not (feature1.startswith(\"trend\") or feature2.startswith(\"trend\"))\n",
    "                        and not (self.feacher_names.__contains__(\n",
    "                            name_to_shortcut[feature1] + \" * \" + name_to_shortcut[feature2])\n",
    "                                 or self.feacher_names.__contains__(\n",
    "                                    name_to_shortcut[feature2] + \" * \" + name_to_shortcut[feature1]))):\n",
    "                    to_add = self.raw_dataset[feature1] * self.raw_dataset[feature2]\n",
    "                    self.raw_dataset[name_to_shortcut[feature1] + \" * \" + name_to_shortcut[feature2]] = to_add\n",
    "                    self.feacher_names.append(name_to_shortcut[feature1] + \" * \" + name_to_shortcut[feature2])\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.data = Data()\n",
    "        pass\n",
    "\n",
    "    def present_data(self, dataset, figure):\n",
    "        plt.figure(figure)\n",
    "        ax = plt.gca()\n",
    "        dataset['dates'] = pd.to_datetime(self.data.dates, format='%Y-%m-%dT%H:%M:%S')\n",
    "        dataset.set_index('dates', inplace=True)\n",
    "        ax = sns.lineplot(data=dataset)\n",
    "        plt.show()\n",
    "\n",
    "    def normalize_data(self):\n",
    "        x = self.data.raw_dataset.values\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        x_scaled = min_max_scaler.fit_transform(x)\n",
    "        self.normalized_dataset = pd.DataFrame(x_scaled)\n",
    "        self.normalized_dataset.columns = self.data.raw_dataset.columns\n",
    "\n",
    "    def reshape_data(self, prediction, ignore, data_point_to_predict=0):\n",
    "        if (ignore != None):\n",
    "            self.data.feacher_names.remove(ignore)\n",
    "\n",
    "        pred = self.normalized_dataset[prediction].copy(deep=True).values\n",
    "        # pred = pred[slice(data_point_to_predict, None)]\n",
    "        pred = pred[slice(None, pred.shape[0] - data_point_to_predict)]\n",
    "        self.dates_prediction = self.data.dates[slice(None, self.data.dates.shape[0] - data_point_to_predict)]\n",
    "        self.prediction = pred.reshape(pred.shape[0], 1)\n",
    "        features = self.normalized_dataset[self.data.feacher_names].values\n",
    "        # features = features[slice(None, features.shape[0] - data_point_to_predict)]\n",
    "        features = features[slice(data_point_to_predict, None)]\n",
    "        self.dates_features = self.data.dates[slice(data_point_to_predict, None)]\n",
    "        self.features = features.reshape(features.shape[0], 1, features.shape[1])\n",
    "\n",
    "    def split_train_test(self, test_size, validation_size=0.1):\n",
    "        relative_val_size = (validation_size / (1 - test_size))  # to make it allways equels to 10% of the data\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.features, self.prediction,\n",
    "                                                                                test_size=test_size, shuffle=False)\n",
    "        date_size = self.X_test.shape[0]\n",
    "        self.dates_prediction = self.dates_prediction[-1 * date_size:]\n",
    "        self.dates_prediction = pd.to_datetime(self.dates_prediction, format='%Y-%m-%dT%H:%M:%S')\n",
    "        self.dates_features = self.dates_features[-1 * date_size:]\n",
    "        self.dates_features = pd.to_datetime(self.dates_features, format='%Y-%m-%dT%H:%M:%S')\n",
    "        self.X_train, self.X_val, self.Y_train, self.Y_val = train_test_split(self.X_train, self.Y_train,\n",
    "                                                                              test_size=relative_val_size,\n",
    "                                                                              shuffle=False)\n",
    "\n",
    "    def rme(self, y_true, y_pred):\n",
    "        return backend.sqrt(abs(backend.mean(backend.square(y_pred - y_true), axis=-1)))\n",
    "\n",
    "    def build_model(self, Nodes=100, LSTM_activation='relu', recurrent_activation='sigmoid', dense_activation='tanh',\n",
    "                    optimizer='adam'):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Bidirectional(LSTM(Nodes, input_shape=(self.features.shape[1], self.features.shape[2]))))\n",
    "        # self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.compile(loss=self.rme, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "\n",
    "    def train_model(self, epochs=30):\n",
    "        history = self.model.fit(self.X_train, self.Y_train, epochs=epochs, verbose=1,\n",
    "                                 validation_data=(self.X_val, self.Y_val))\n",
    "        return (history.history)\n",
    "\n",
    "    def test_model(self, test_size=0.1):\n",
    "        self.Predict = self.model.predict(self.X_test)\n",
    "        score = self.model.evaluate(self.X_test, self.Y_test)\n",
    "        names = self.model.metrics_names\n",
    "        score_dic = {}\n",
    "        for i in range(0, len(names)):\n",
    "            score_dic[names[i]] = score[i]\n",
    "        plt.figure(2)\n",
    "        plt.scatter(self.Predict, self.Y_test)\n",
    "        plt.show(block=False)\n",
    "\n",
    "        plt.subplots(figsize=(11, 11))\n",
    "        plt.figure(3)\n",
    "        plt.xticks(rotation='vertical')\n",
    "        Test, = plt.plot(self.dates_features, self.Y_test)\n",
    "        Predict, = plt.plot(self.dates_prediction, self.Predict)\n",
    "        plt.legend([Test, Predict], [\"Real Data\", \"Predicted Data\"])\n",
    "        plt.title('test size =' + str(test_size))\n",
    "        plt.show(block=False)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, sharey=True)\n",
    "        ax1.plot(self.dates_features, self.Y_test)\n",
    "        ax1.set(title=\"Real Data\")\n",
    "        ax2.plot(self.dates_prediction, self.Predict)\n",
    "        ax2.set(title=\"Predicted data\")\n",
    "        plt.show()\n",
    "        return (score_dic)\n",
    "\n",
    "\n",
    "def run(args):\n",
    "    sns.set()\n",
    "    m = Model()\n",
    "    m.data.fetch_data(args.path)\n",
    "    m.present_data(m.data.raw_dataset, 1)\n",
    "    m.data.add_features(args.data_point_to_predict, args.prediction)\n",
    "    m.data.heat_map()\n",
    "    m.normalize_data()\n",
    "    # m.present_data(m.normalized_dataset, 2)\n",
    "    m.reshape_data(args.prediction, args.ignore, args.data_point_to_predict)\n",
    "    m.split_train_test(args.test_size)\n",
    "    m.build_model()\n",
    "    m.train_model()\n",
    "    m.test_model(test_size=args.test_size)\n",
    "\n",
    "\n",
    "def evaluate(args):\n",
    "    test_size_arr = np.linspace(0.1, 0.9, 8, endpoint=False)\n",
    "    m = Model()\n",
    "    m.data.fetch_data(args.path)\n",
    "    m.normalize_data()\n",
    "    m.reshape_data(args.prediction, args.ignore, args.data_point_to_predict)\n",
    "    name_of_file = args.path.replace(os.path.sep + 'data', '')\n",
    "    name_of_file = name_of_file.replace(os.path.sep, '-')\n",
    "    with open('evaluation/' + name_of_file + '.csv', \"w\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(['test_size', 'val_loss', 'val_mse', 'val_mae', 'score_loss', 'score_mse', 'score_mae'])\n",
    "        for test_size in test_size_arr:\n",
    "            m.split_train_test(test_size)\n",
    "            m.build_model()\n",
    "            history = m.train_model()\n",
    "            score = m.test_model(test_size)\n",
    "            writer.writerow(\n",
    "                [test_size, history['val_loss'][29], history['val_mse'][29], history['val_mae'][29], score['loss'],\n",
    "                 score['mse'], score['mae']])\n",
    "        csv_file.close()\n",
    "\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "    parser = argparse.ArgumentParser(description='This is an LSTM model to detect anomalies in data for Taboola')\n",
    "    parser.add_argument('-path', action='store', dest='path')\n",
    "    parser.add_argument('-prediction', action='store', dest='prediction')\n",
    "    parser.add_argument('-test_size', action='store', dest='test_size', type=float)\n",
    "    parser.add_argument('-ignore', action='store', dest='ignore', default=None)\n",
    "    parser.add_argument('-predict_amount', action='store', dest='data_point_to_predict', type=int, default=0)\n",
    "    args = parser.parse_args()\n",
    "    run(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}